{"cells":[{"cell_type":"code","source":["RUN_TESTS = False"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9891fbe-c784-451c-9cbb-4dbbbf37e025"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ./utils"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Run the utils notebook ","showTitle":true,"inputWidgets":{},"nuid":"47f20883-1e0a-4422-a5f0-44d0eb20ad0e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;2.4.1&#39;, [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:1&#39;, device_type=&#39;GPU&#39;)]]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;2.4.1&#39;, [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:1&#39;, device_type=&#39;GPU&#39;)]]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ./architectures"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a97e4c2-b07b-430c-a792-845525caa274"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Base Autoencoder Class\nThis the main parent class for the autoencoder which will be used as a base class for VAE and AE model classes \n\n### Class attributes\n- model_name: Name of the model\n- encoder_layers_config: A list of dictionaries that defines the architecture for encoder\n- decoder_layers_config: A list of dictionaries that defines the architecture for decoder\n- original_dim: The input dimension for the NN\n- latent_dim: The dimension for latent space\n- hvd_flag: Boolean flag to enable horovod distribution\n\n### Class methods\n- fit: This method is derived from the keras model fit and uses either horovod ft or the regular fit ased on the hvd_flag\n- predict: This method is derived from the keras model predict\n- summary: This method is derived from the keras model summary. It prints out the model architecture\n- compile: This method is derived from the keras model compile. It gives an option to compile it using the hvd distributor\n- create_checkpoint_cb: To create callback for model checkpoints to be used in fit method\n- load_weights: Load model weights from checkpoint directory\n- latest_checkpoint: Load the latest model checkpoint\n- save_model: Save the model to a directory\n- load_model: Load the model from a directory\n- get_tensorflow_callback: Returns tensorboard callbacks\n\n### Internal methods\n- init_hvd: To initialize horovod distribution framework\n- emit_config: TBD\n- compile_hvd: Compile method to be used for hvd compile and called in the class compile method\n- fit_hvd: Fit method for hvd distribution used in the class fit method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec64814b-9014-4808-b798-18662383ed22"}}},{"cell_type":"code","source":["class AutoEncoder(NnUtils):\n  def __init__(self, architecture, experiment_name, latent_dim, hvd_flag=False, root_dir=\"/dbfs/FileStore/jf-cache/models\"):\n    \n    self.encoder_layers_config = architecture.encoder\n    self.decoder_layers_config = architecture.decoder\n    \n    self.experiment_name = experiment_name\n    self.model_name = NnUtils._postfix(architecture.name)\n    \n    self.original_dim = architecture.stimulus_size\n    self.latent_dim = latent_dim\n    \n    self.hvd_flag = hvd_flag\n    \n    self.root_dir = f\"{root_dir}/{self.experiment_name}/{self.model_name}\"\n    \n    os.makedirs(self.root_dir, exist_ok=True)\n    os.makedirs(f\"{self.root_dir}/candidates\", exist_ok=True)\n    \n    if self.hvd_flag:\n      self.__init_hvd__()\n      \n  def _emit_config(self):\n    \n    args = (self.model_name, self.encoder_layers_config, self.decoder_layers_config, self.hvd_flag)\n    kwargs = {\"original_dim\": self.original_dim,\n              \"latent_dim\": self.latent_dim}\n    \n    return args, kwargs\n  \n  def __init_hvd__(self):\n    # Initialize Horovod\n    hvd.init()\n\n    # Pin GPU to be used to process local rank (one GPU per process)\n    # These steps are skipped on a CPU cluster\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if len(gpus) > 0:\n      try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n      except Exception as e:\n        print(\"Memory Growth Already Configured\")\n    if gpus:\n      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n  \n  def compile(self, optimizer='adam', loss=None, metrics=[]):\n    if self.hvd_flag:\n      self._compile_hvd(loss=loss, metrics=metrics)\n    else:\n      self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n      \n    # Create Checkpoint Directory Ready For Model Fit\n    self.create_checkpoint_cb()\n  \n  def _compile_hvd(self, loss=None, metrics=[]):\n    # Adjust learning rate based on number of GPUs\n    optimizer = keras.optimizers.Adam(lr=.01 * hvd.size())\n    # Use the Horovod Distributed Optimizer\n    optimizer = hvd.DistributedOptimizer(optimizer)\n    self.model.compile(optimizer=optimizer,\n                       loss=loss,\n                       metrics=metrics)\n      \n  def __repr__(self):\n    #self.model.summary()\n    pass\n    \n  def fit(self, *args, **kwargs):\n    if self.hvd_flag:\n      # Pass epochs to this function (its inside kwargs)\n      self._fit_hvd(epochs=1)\n    else:\n      self.model.fit(*args, **kwargs)\n    \n  def _fit_hvd(self, epochs=1):\n    \n    args, kwargs = self._emit_config()\n    \n    this_class = type(self).__name__\n    \n    image = type(self.original_dim) is tuple\n    \n    def train():\n      mnist_data = get_data_mnist(flat=(not image))\n      if this_class == \"AE\":\n        model_obj = AE(*args, **kwargs)\n      elif this_class == \"VAE\":\n        model_obj = VAE(*args, **kwargs)\n      else:\n        return False\n      \n      model_obj.model.fit(x=mnist_data[\"images\"][\"train\"], \n                          y=mnist_data[\"images\"][\"train\"], \n                          epochs=epochs, \n                          validation_data=(mnist_data[\"images\"][\"test\"], mnist_data[\"images\"][\"test\"]),\n                          callbacks = [model_obj.cp_callback])\n      \n    hr = HorovodRunner(np=2)\n    hr.run(train)\n    \n    # Sort this so it gets the latest weights\n    self.model.load_weights(os.path.dirname(f'{self.root_dir}/{self.model_name}/training_1/cp.ckpt'))\n    \n  def predict(self, *args, **kwargs):\n    return self.model.predict(*args, **kwargs)\n    \n  def summary(self):\n    return self.model.summary()\n  \n  def create_checkpoint_cb(self):\n    \n    self.ckpt_file_name = f\"{self.root_dir}/training_1/cp.ckpt\"\n    \n    os.makedirs(self.ckpt_file_name, exist_ok=True)\n    \n    self.checkpoint_dir = os.path.dirname(self.ckpt_file_name)\n    \n    # Create a callback that saves the model's weights\n    # TD - Add in the call backs for saving the CVSlogger\n    self.cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.checkpoint_dir,\n                                                          save_weights_only=True,\n                                                          verbose=1)\n    \n    ## CODE FOR LATER - KEEP\n    #callbacks = [tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', \n    #                                              mode='min', save_weights_only=True,\n    #                                              save_best_only=True),\n    #          tf.keras.callbacks.CSVLogger(tmp_path, separator=',', append=True)]\n  \n  def load_weights(self, *args, **kwargs):\n    self.model = self.model.load_weights(*args, **kwargs)\n    \n  def save_weights(self, *args, **kwargs):\n    self.model.save_weights(*args, **kwargs)\n  \n  def latest_checkpoint(self,*args, **kwargs):\n    return tf.train.latest_checkpoint(*args, **kwargs)\n  \n  def save_model(self):\n    \n    print(f\"SAVING: {self.model_name}\")\n    \n    args, kwargs = self._emit_config()\n    with open(f\"{self.root_dir}/model_config.pkl\", 'wb') as file_handle:\n      pickle.dump({\"args\": args, \"kwargs\": kwargs}, file_handle)\n    \n    self.save_weights(f\"{self.root_dir}/latest_weights\")\n    \n    #os.makedirs(f\"/dbfs/FileStore/gb-cache/models/{self.model_name}/\", exist_ok=True)\n    \n    #self.model.save(f\"/dbfs/FileStore/gb-cache/models/{self.model_name}/ae-model\")\n    \n  def load_model(self):\n      \n    if self.to_load:\n      with open(f\"{self.root_dir}/model_config.pkl\", 'rb') as file_handle:\n        args_kwargs = pickle.load(file_handle)\n      \n      self.__init__(*args_kwargs['args'], **args_kwargs['kwargs'])\n      \n      self.load_weights(f\"{self.root_dir}/latest_weights\")\n      \n      self.to_load = False\n    else:\n      self.to_load = True\n    \n\n    #print(f\"LOADING: {self.model_name}\")\n    \n    #with open(f\"/dbfs/FileStore/gb-cache/models/{self.model_name}/class_{self.model_type}.pkl\", \"rb\") as file_handle:\n    #  load_class_dict = pickle.load(file_handle)\n      \n    #class_dict = self.__dict__\n    \n    #for k, v in load_class_dict.items():\n    #  class_dict[k] = v\n      \n    #self.__dict__ = class_dict\n    \n    #self.encoder = tf.keras.models.load_model(f\"/dbfs/FileStore/gb-cache/models/{self.model_name}/encoder-model\")\n    #self.decoder = tf.keras.models.load_model(f\"/dbfs/FileStore/gb-cache/models/{self.model_name}/decoder-model\")\n    #self.vae = tf.keras.models.load_model(f\"/dbfs/FileStore/gb-cache/models/{self.model_name}/ae-model\")\n\n  def get_tensorflow_callback(self, experiment_log_dir):\n    \n    os.makedirs(experiment_log_dir, exist_ok=True)\n\n    run_log_dir = f\"{experiment_log_dir}/{self.model_name}__{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n    return tf.keras.callbacks.TensorBoard(log_dir=run_log_dir, histogram_freq=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Base Autoencoder Class","showTitle":true,"inputWidgets":{},"nuid":"a0eb4a97-7390-45df-b332-623a209a247b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Architectures\n\n### VGG Architectures\n\nVisual Geometry Group\n\n[Oxford Group](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n\n[Publication](https://www.robots.ox.ac.uk/~vgg/publications/2015/Simonyan15/simonyan15.pdf)\n\n### MLP Architectures\n\nMultilayer Perceptron\n\n[wiki - MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa8e580f-7fe7-488e-8252-43c4d2c26736"}}},{"cell_type":"markdown","source":["## Encoder and Decoders\n\nEncoders make up the input (encoder) and output (decoder) modules of an autoecoder. The encoder is designed to reduce the dimensionality of the the network input through progressive layers to end provide a compressed representation of the original input to the latent space.\n\nDecoders do the opposite operation, expanding the dimensionality of the activity of the latent space so that through progressive layers of the decoder the output of the final layer has the same dimensions as the original input.\n\nThrough training, the reconstruction loss drives the weights of the network to minimise the difference between the input to the encoder and the output of the decoder.\n\n## Links\n\n[wiki - Autoencoders](https://en.wikipedia.org/wiki/Autoencoder)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89bba61d-f2fb-4943-9200-28999b2a7798"}}},{"cell_type":"code","source":["class _encoder(NnUtils):#, tkm.Model):\n  def __init__(self, input_dim, latent_dim, layers_config, name, heads=1, build_inputs=True):\n    \n    super().__init__()\n    \n    self.input_dim = input_dim\n    self.latent_dim = latent_dim\n    self.layers_config = layers_config\n    self.heads = heads\n    \n    self.name = name\n    \n    if build_inputs:\n      self.input = [self.create_basic_inputs(self.input_dim, f\"{name}_inputs\") for i in range(self.heads)]\n      self.penultimate = self.up_to_penultimate()\n    else:\n      self.input = None\n      self.penultimate = None\n      \n    def predict(self, *args, **kwargs):\n      return self.model.predict(*args, **kwargs)\n\n  def up_to_penultimate(self):\n    if len(self.input) > 1:\n      return tkl.Concatenate(axis=-1)([ self.get_layers(input_, self.layers_config) for input_ in self.input])\n    else:\n      return self.get_layers(self.input[0], self.layers_config)\n      \n    \n  def predict(self, *args, **kwargs):\n    return self.model.predict(*args, **kwargs)\n    \n    \nclass ae_encoder(_encoder):\n  def __init__(self, *args, **kwargs):\n#     s_kwargs = kwargs.copy()\n#     for k in ['last_activation']:#, 'build_model']:\n#       if k in s_kwargs:\n#         del s_kwargs[k]\n#     super().__init__(*args, **s_kwargs)\n    \n#     #print(kwargs)\n    \n#     #if kwargs['build_model']:\n#     kwargs = {}\n#     self.__build__(**kwargs)\n    self.build_arguments = []\n    \n    build_kwargs, init_kwargs = self.split_kwargs(kwargs, self.build_arguments)\n    \n    super().__init__(*args, **init_kwargs)\n    self.__build__(**build_kwargs)\n    \n  def __build__(self):\n    \n    self.output = tkl.Dense(self.latent_dim, activation='linear')(self.penultimate)\n    \n    self.model = tkm.Model(self.input, self.output)\n    \n  def __call__(self, x):\n    \n    return self.model(x)\n    \n\nclass vae_encoder(ae_encoder):\n  def __init__(self, *args, **kwargs):\n#     s_kwargs = kwargs.copy()\n#     for k in ['last_activation']:#, 'build_model']:\n#       if k in s_kwargs:\n#         del s_kwargs[k]\n#     super().__init__(*args, **s_kwargs)\n    \n#     #if kwargs['build_model']:\n#     kwargs = {}\n#     self.__build__(**kwargs)\n    self.build_arguments = []\n    \n    build_kwargs, init_kwargs = self.split_kwargs(kwargs, self.build_arguments)\n    \n    super().__init__(*args, **init_kwargs)\n    self.__build__(**build_kwargs)\n  \n  def __build__(self):\n    \n    self.z_mean, self.z_log_sigma = self.variational_layers(self.penultimate)\n    \n    self.output = tkl.Lambda(self.sampling)([self.z_mean, self.z_log_sigma])\n    \n    self.model = tkm.Model(self.input, self.output)    \n  \n  def __call__(self, x):\n    \n    return self.model(x)\n\n  def variational_layers(self, h):\n    \n    return tkl.Dense(self.latent_dim)(h), tkl.Dense(self.latent_dim)(h)\n\n  def sampling(self, args):\n    z_mean, z_log_sigma = args\n    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.latent_dim),\n                              mean=0.0, \n                              stddev=0.1)\n    return z_mean + K.exp(z_log_sigma) * epsilon\n  \n  def get_variational_loss(self, inputs, outputs):\n    reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n    \n    if len(reconstruction_loss.shape) > 1:\n      reconstruction_loss = K.sum(K.sum(reconstruction_loss, axis=-1), axis=-1)\n      \n    #print(reconstruction_loss)\n    \n    kl_loss = 1 + self.z_log_sigma - K.square(self.z_mean) - K.exp(self.z_log_sigma)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n\n    #print(kl_loss)\n    \n    return K.mean(reconstruction_loss + kl_loss)\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Encoder Modules","showTitle":true,"inputWidgets":{},"nuid":"5f8e0db2-2023-4432-add0-ab3dcd9683c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class _decoder(NnUtils):\n  def __init__(self, input_dim, output_dim, layers_config, name, build_inputs=True):\n    \n    super().__init__()\n    \n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.layers_config = layers_config\n    \n    self.name = name\n    \n    if build_inputs:\n      self.input = self.create_basic_inputs(self.input_dim, f\"{name}_input\")\n    else:\n      self.input = None\n      \n  def predict(self, *args, **kwargs):\n    return self.model.predict(*args, **kwargs)\n\n\nclass ae_decoder(_decoder):\n  def __init__(self, *args, **kwargs):\n    \n    self.build_arguments = ['last_activation']\n    \n    build_kwargs, init_kwargs = self.split_kwargs(kwargs, self.build_arguments)\n    \n    super().__init__(*args, **init_kwargs)\n    self.__build__(**build_kwargs)\n    \n  def __build__(self, last_activation='linear'):\n    x = self.get_layers(self.input, self.layers_config)\n    \n    if type(self.output_dim) is tuple:\n      ### TD - HACK\n      self.output = x\n    else:\n      self.output = tkl.Dense(self.output_dim, activation=last_activation)(x)\n    \n    self.model = tkm.Model(self.input, self.output)    \n    \n  def __call__(self, x):\n    \n    return self.model(x)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Decoder Modules","showTitle":true,"inputWidgets":{},"nuid":"b7ac8377-b8c0-4d9a-b798-3eedf4aa8be9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class AE(AutoEncoder):\n  \"\"\"\n  The final child class of Autoencoders\n  AE inherits from AutoEncoder inherits from NnUtils\n\n  Executes the final build of the Autoencoder class by combining Encoder and Decoder Modules\n  \n  Features:\n  - Accepts an Arcitecture class to specifiy the configuration for the Encoder and Decoder modules\n  - Can have many heads (and tails) to process many heatmaps for the same play\n    - This works by having an input of the form:\n      Example:\n        n_samples = n\n        size of image = l by m\n        depth of image = 1 (one is the only ption for the current implementation)\n        This list would serve as input for a netowkr with 4 heads\n        [ np.ndarray(shape=(n,l,m,1)), np.ndarray(shape=(n,l,m,1)), np.ndarray(shape=(n,l,m,1)), np.ndarray(shape=(n,l,m,1))]\n\n  ...\n\n  Attributes\n  ----------\n   - head_names: The names of the heatmaps that are used as input for the network\n   - head_count: The number of heatmaps per play (per sample)\n   - \n\n  Methods\n  -------\n   - dense_reshape - Connects a layer to a dense fully connected linear layer and then reshapes to the desired output dims\n   - _postfix - Adds a short random string to a string - Used to prevent name conflicts within a network\n   - split_kwargs - Splits a dictionary (kwargs) in to two dictionaries according to if the key is present in a list (args_list) \n   - get_layers - Creates a series of connected layers according to the dictionary layers_config\n   - create_basic_inputs - Creates an Input layer with shape: input_dim and name: name with a post-fix\n  \"\"\"\n  def __init__(self, architecture, experiment_name, heads, latent_dim, hvd_flag=False, last_activation='linear', variational=False):\n    super().__init__(architecture, experiment_name, latent_dim, hvd_flag=hvd_flag)\n    \n    self.head_names = heads\n    self.head_count = len(heads)\n    \n    # TD - GB\n    if variational:\n      encoder_, decoder_ = self.get_modules_config(\"vae\")\n    else:\n      encoder_, decoder_ = self.get_modules_config(\"ae\")\n    \n    self.encoder = encoder_['class'](self.original_dim, self.latent_dim, self.encoder_layers_config, encoder_['name'], \n                                     heads=self.head_count)\n    self.decoder = [ decoder_['class'](self.latent_dim, self.original_dim, self.decoder_layers_config, decoder_['name'], \n                                       last_activation=last_activation) for i in self.head_names]\n    \n    self.input = self.encoder.input\n    if self.head_count == 1:\n      self.decoder = self.decoder[0]\n      self.output = self.decoder(self.encoder.output)\n    else:\n      self.output = [ x(self.encoder.output) for x in self.decoder]\n    self.model = tkm.Model(self.input, self.output)\n    \n    if variational:\n      self.model.add_loss(self.encoder.get_variational_loss(self.input, self.output))\n      self.compile(optimizer='adam', loss=None, metrics=['accuracy'])\n    else:\n      self.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n    \n  @staticmethod\n  def get_modules_config(network_type):\n    if network_type == \"ae\":\n      return {\"class\": ae_encoder, \"name\": \"ae_encoder\"}, {\"class\": ae_decoder, \"name\": \"ae_decoder\"}\n    elif network_type == \"vae\":\n      return {\"class\": vae_encoder, \"name\": \"vae_encoder\"}, {\"class\": ae_decoder, \"name\": \"vae_decoder\"}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Autoencoder Child Classes - For Each Type of Autoencoder","showTitle":true,"inputWidgets":{},"nuid":"9015f882-86f9-4f3c-98f6-0ced3e81cb8e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def show_results(mnist_data, model_obj, many=False):\n  n_test_samples = len(mnist_data[\"images\"][\"test\"])\n  index = np.random.randint(n_test_samples)\n  original_image = mnist_data[\"images\"][\"test\"][[index]]\n  \n  if many:\n    reconstructed_image = model_obj.predict([mnist_data[\"images\"][\"test\"][[index]] for i in range(3)])[0][0]\n  else:\n    reconstructed_image = model_obj.predict(mnist_data[\"images\"][\"test\"][[index]])[0]\n\n  original_image = original_image.reshape(28,28)\n  reconstructed_image = reconstructed_image.reshape(28,28)\n  \n  #plot_reconstruction(original_image, reconstructed_image)\n  \n  \ndef test_ae(model_name, architecture, latent_dim = 2, \n            hvd_flag=False, epochs=32, heads=['position'], variational=False): \n  \n  mnist_data = get_data_mnist(flat=(type(architecture.stimulus_size) is int))\n  \n  #ae = AE(architecture, heads, latent_dim, hvd_flag=hvd_flag, variational=variational)\n  \n  #jf add\n  experiment_name = None\n  ae = AE(architecture, experiment_name, heads, latent_dim, hvd_flag=hvd_flag, variational=variational)\n  \n  if hvd_flag:\n    ae.fit(epochs=epochs)\n  else:\n    \n    if ae.head_count == 1:\n      x = mnist_data[\"images\"][\"train\"]\n      y = mnist_data[\"images\"][\"train\"]\n      x_ = mnist_data[\"images\"][\"train\"]\n      y_ = mnist_data[\"images\"][\"train\"]\n    else:\n      x  = [mnist_data[\"images\"][\"train\"] for i in heads]\n      y  = [mnist_data[\"images\"][\"train\"] for i in heads]\n      x_ = [mnist_data[\"images\"][\"train\"] for i in heads]\n      y_ = [mnist_data[\"images\"][\"train\"] for i in heads]\n    \n    ae.fit(x=x, \n           y=y, \n           epochs=epochs, \n           validation_data=(x_, y_),\n           callbacks = [ae.cp_callback])\n  \n    ae.save_model()\n  \n  show_results(mnist_data, ae, many=(ae.head_count>1))\n  \n  return ae\n\n\ndef batch_tests(test_epochs, test_heads, test_hvd):\n  \n  # Test for: Single Node Training, VGG Autoencoder\n  model_name_ae_vgg_single = NnUtils._postfix(\"TEST_MR_V5\")\n  ae_vgg_single = test_ae(model_name_ae_vgg_single, vgg_basic_architecture, \n                          epochs=test_epochs, hvd_flag=test_hvd, heads=test_heads, variational=False)\n\n  # Test for: Single Node Training, VGG Autoencoder with a variational latent space\n  model_name_vae_vgg_single = NnUtils._postfix(\"TEST_MR_V5\")\n  vae_vgg_var_single = test_ae(model_name_vae_vgg_single, vgg_basic_architecture, \n                               epochs=test_epochs, hvd_flag=test_hvd, heads=test_heads, variational=True)\n\n  # Test for: Single Node Training, MLP Autoencoder\n  model_name_ae_mlp_single = NnUtils._postfix(\"TEST_MR_V5\")\n  ae_mlp_single = test_ae(model_name_ae_mlp_single, mlp_basic_architecture, \n                          epochs=test_epochs, hvd_flag=test_hvd, heads=test_heads, variational=False)\n\n  # Test for: Single Node Training, MLP Autoencoder with a variational latent space\n  model_name_vae_mlp_single = NnUtils._postfix(\"TEST_MR_V5\")\n  ae_mlp_var_single = test_ae(model_name_vae_mlp_single, mlp_basic_architecture, \n                              epochs=test_epochs, hvd_flag=test_hvd, heads=test_heads, variational=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Test Functions","showTitle":true,"inputWidgets":{},"nuid":"7f1cd92e-0824-43ef-9226-1e127661e15f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if RUN_TESTS:\n  test_epochs = 1\n  test_heads = ['position']\n  test_hvd = False\n\n  batch_tests(test_epochs, test_heads, test_hvd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Tests for single node training - Single Head","showTitle":true,"inputWidgets":{},"nuid":"e9abdc6e-5849-43ba-ab4d-422b48851795"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if RUN_TESTS:\n  test_epochs = 1\n  test_heads = ['position','speed']\n  test_hvd = False\n\n  batch_tests(test_epochs, test_heads, test_hvd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Tests for single node training - Many Heads","showTitle":true,"inputWidgets":{},"nuid":"2f3eb599-dc63-4e9d-82cf-221f6d03c8af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if RUN_TESTS:\n  test_epochs = 1\n  test_heads = ['position']\n  test_hvd = True\n\n  batch_tests(test_epochs, test_heads, test_hvd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Tests for distributed training - Single Head","showTitle":true,"inputWidgets":{},"nuid":"fcf20fed-f999-4f6e-bb5a-68efd7556ede"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if RUN_TESTS:\n  test_epochs = 1\n  test_heads = ['position','speed']\n  test_hvd = True\n\n  batch_tests(test_epochs, test_heads, test_hvd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Tests for distributed training - Many Heads","showTitle":true,"inputWidgets":{},"nuid":"74ab52f9-a1e4-4c1a-8c99-8dce19ec90d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#ae_mlp.chk_dir\n#dbutils.fs.ls(f'/FileStore/gb-cache/models/{model_name_single_node}/')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c51471ce-5115-4d9e-8aa6-5135675dc2d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"base_autoencoder","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2958729778901687}},"nbformat":4,"nbformat_minor":0}